

# ğŸš€ Kagent + Ollama (Fully Local Only Setup)

This guide installs:

* âœ… Kagent
* âœ… Ollama
* âœ… tinyllama model
* âŒ No OpenAI
* âŒ No Gemini

Everything runs locally inside your cluster.

---

# ğŸ“Œ Requirements

* Kubernetes cluster
* Helm
* kubectl
* 4GB RAM minimum (use tinyllama)

---

# ğŸŸ¢ STEP 1 â€” Create Namespace

```bash
kubectl create namespace kagent
```

---

# ğŸŸ¢ STEP 2 â€” Install CRDs

```bash
helm install kagent-crds oci://ghcr.io/kagent-dev/kagent/helm/kagent-crds \
  --namespace kagent
```

---

# ğŸŸ¢ STEP 3 â€” Install Kagent (OLLAMA ONLY)

Create values file:

```bash
nano kagent-values.yaml
```

Paste this:

```yaml
providers:
  openAI:
    enabled: false

  gemini:
    enabled: false

  ollama:
    enabled: true
    baseUrl: http://ollama:11434
```

Save.

Now install:

```bash
helm install kagent oci://ghcr.io/kagent-dev/kagent/helm/kagent \
  --namespace kagent \
  --values kagent-values.yaml
```

Wait:

```bash
kubectl get pods -n kagent
```

---

# ğŸŸ¢ STEP 4 â€” Deploy Ollama

Create:

```bash
nano ollama.yaml
```

Paste:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-pvc
  namespace: kagent
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp2
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: kagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: kagent
spec:
  selector:
    app: ollama
  ports:
    - port: 11434
      targetPort: 11434
  type: ClusterIP
```

Apply:

```bash
kubectl apply -f ollama.yaml
```

Wait until:

```bash
kubectl get pods -n kagent
```

Ollama should be `Running`.

---

# ğŸŸ¢ STEP 5 â€” Pull Local Model

For 4GB RAM (recommended):

```bash
kubectl exec -it -n kagent deploy/ollama -- ollama pull tinyllama
```

Verify:

```bash
kubectl exec -it -n kagent deploy/ollama -- ollama list
```

You must see:

```
tinyllama:latest
```

---

# ğŸŸ¢ STEP 6 â€” Create Ollama ModelConfig

Create:

```bash
nano ollama-model.yaml
```

Paste:

```yaml
apiVersion: kagent.dev/v1alpha2
kind: ModelConfig
metadata:
  name: ollama-model-config
  namespace: kagent
spec:
  model: tinyllama:latest
  provider: Ollama
  ollama: {}
```

Apply:

```bash
kubectl apply -f ollama-model.yaml
```

Restart controller:

```bash
kubectl rollout restart deployment kagent-controller -n kagent
```

---

# ğŸŸ¢ STEP 7 â€” Access UI

```bash
kubectl port-forward -n kagent svc/kagent-ui 8080:8080
```

Open:

```
http://localhost:8080
```

---

# ğŸŸ¢ STEP 8 â€” Create Agent

Create new agent.

Select:

```
Ollama (tinyllama:latest)
```

Type:

```
hi
```

It should respond.

---

# ğŸ‰ Final Result

You now have:

* Fully Local AI
* No OpenAI
* No Gemini
* No quota limits
* No billing
* 100% inside Kubernetes

---

# ğŸ›  If Something Fails

Check:

```bash
kubectl logs -n kagent deploy/ollama
```

Check controller:

```bash
kubectl logs -n kagent deploy/kagent-controller
```

Test manually:

```bash
kubectl run debug --rm -it \
  --image=curlimages/curl \
  --namespace kagent \
  --restart=Never \
  --command -- sh
```

Inside:

```bash
curl -X POST http://ollama:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyllama:latest","messages":[{"role":"user","content":"hello"}]}'
```

---

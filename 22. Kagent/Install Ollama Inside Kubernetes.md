
* âœ… AWS EC2
* âœ… Kubernetes cluster
* âœ… kagent deployed
* âŒ External API quota issue
* ğŸ¯ Now you want â†’ **Fully Local LLM using Ollama**

This is the BEST decision. No 429 errors. No billing stress.

---

# ğŸ—ï¸ Architecture We Will Build

```
Kubernetes (EKS or self-managed)
        â”‚
        â”‚
   kagent pods
        â”‚
        â”‚ (HTTP)
        â–¼
   Ollama Server (inside cluster)
        â”‚
        â–¼
   Local Model (llama3 / mistral / phi3)
```

Everything runs inside your cluster ğŸš€

---

# ğŸš€ STEP 1 â€” Install Ollama Inside Kubernetes

We will deploy Ollama as a pod.

---

## ğŸŸ¢ Create Ollama Deployment

Create file:

```bash
nano ollama-deployment.yaml
```

Paste this:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      volumes:
        - name: ollama-data
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
spec:
  selector:
    app: ollama
  ports:
    - port: 11434
      targetPort: 11434
  type: ClusterIP
```

Save and exit.

---

## ğŸŸ¢ Apply It

```bash
kubectl apply -f ollama-deployment.yaml
```

Check:

```bash
kubectl get pods
```
- Wait 5 mins And then cheque it
Wait until:

```
ollama-xxxx    Running
```

- ğŸš€ If You Want To Check Image Pull Progress
Run:
```bash
kubectl describe pod ollama-6747f4c5f4-k7kbg | grep -A5 Events
```
If it changes to:
```bash
Pulled
Created
Started
```
Then it's done.


```bash
kubectl get pods
```
```
ollama-xxxx    Running
```



---

# ğŸš€ STEP 2 â€” Download Model Inside Ollama Pod

Enter pod:

```bash
kubectl exec -it deploy/ollama -- bash
```

Now pull model:

```bash
ollama pull llama3
```

OR smaller (better for EC2 without GPU):

```bash
ollama pull phi3
```

âš ï¸ If your EC2 has no GPU and small RAM:
Use `phi3` (lightweight)

Exit pod:

```bash
exit
```

---

# ğŸš€ STEP 3 â€” Test Ollama From Cluster

Run:

```bash
kubectl exec -it deploy/ollama -- ollama run phi3
```

Type:

```
hello
```

If it responds â†’ âœ… Ollama working

---

# ğŸš€ STEP 4 â€” Configure kagent To Use Ollama

Now we must tell kagent:

âŒ Donâ€™t use OpenAI
âŒ Donâ€™t use Gemini
âœ… Use Ollama

---

## ğŸ” Find kagent config

Check:

```bash
kubectl get configmap
kubectl get secrets
```

Look for something like:

```
kagent-config
```

---

## ğŸŸ¢ Update Environment Variables

Edit kagent deployment:

```bash
kubectl edit deployment my-first-k8s-agent
```

Inside container section, add:

```yaml
env:
  - name: MODEL_PROVIDER
    value: "ollama"
  - name: OLLAMA_BASE_URL
    value: "http://ollama:11434"
  - name: OLLAMA_MODEL
    value: "phi3"
```

Save and exit.

---

# ğŸš€ STEP 5 â€” Restart kagent

```bash
kubectl rollout restart deployment my-first-k8s-agent
```

Check logs:

```bash
kubectl logs -f deploy/my-first-k8s-agent
```

If no more 429 errors â†’ SUCCESS ğŸ‰

---


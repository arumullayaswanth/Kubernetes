
# âŒ External API Quota Issue

# ğŸ¯ Solution â†’ Fully Local LLM using Ollama

If you see errors like:

```
429 - insufficient_quota
```

That means:

* OpenAI quota finished
* Billing issue
* API limits reached

Instead of depending on external APIsâ€¦

ğŸ‘‰ We switch to **Fully Local LLM using Ollama**

### Why This Is Better

* âœ… No 429 errors
* âœ… No billing stress
* âœ… No internet dependency
* âœ… Fully private
* âœ… Runs inside your cluster

---

# ğŸ—ï¸ Architecture We Will Build

```
Kubernetes (EKS / Minikube / etc.)
        â”‚
        â”‚
   kagent pods
        â”‚
        â”‚  (HTTP)
        â–¼
   Ollama Service (ClusterIP)
        â”‚
        â–¼
   Local Model (tinyllama / phi3)
```

Everything runs inside your Kubernetes cluster ğŸš€

---

# ğŸš€ STEP 1 â€” Deploy Ollama Inside Kubernetes

We deploy Ollama as a normal Kubernetes deployment.

---

## ğŸŸ¢ Create Ollama Deployment

Create file:

```bash
vim ollama.yaml
```

Paste:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-pvc
  namespace: kagent
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp2   # change if needed
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: kagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: kagent
spec:
  selector:
    app: ollama
  ports:
    - port: 11434
      targetPort: 11434
  type: ClusterIP
```

Save and exit.

---

## ğŸŸ¢ Apply Ollama

```bash
kubectl apply -f ollama.yaml
```

Watch PVC:

```bash
kubectl get pvc -n kagent -w
```

It should change:

```
Pending â†’ Bound
```

Check pod:

```bash
kubectl get pods -n kagent
```

Wait until:

```
ollama-xxxx   Running
```

---

# ğŸš€ STEP 2 â€” Pull Model (Very Important)

âš  If your instance has 4GB RAM â†’ use tinyllama
âš  If your instance has 8GB+ RAM â†’ you can use phi3

### Recommended for Students (4GB):

```bash
kubectl exec -it -n kagent deploy/ollama -- ollama pull tinyllama
```

Verify:

```bash
kubectl exec -it -n kagent deploy/ollama -- ollama list
```

You should see:

```
tinyllama:latest
```

---

# ğŸš€ STEP 3 â€” Test Ollama From Inside Cluster

Run debug pod:

```bash
kubectl run debug --rm -it \
  --image=curlimages/curl \
  --namespace kagent \
  --restart=Never \
  --command -- sh
```

Inside:

```bash
curl -X POST http://ollama:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyllama:latest","messages":[{"role":"user","content":"hello"}]}'
```

If you see streaming JSON â†’ Ollama is working âœ…

Exit debug pod.

---

# ğŸš€ STEP 4 â€” Create Ollama ModelConfig (Correct Way)

âš  Do NOT edit deployment manually
âš  Do NOT set environment variables manually

We use ModelConfig.

Create file:

```bash
vim ollama-model.yaml
```

Paste:

```yaml
apiVersion: kagent.dev/v1alpha2
kind: ModelConfig
metadata:
  name: ollama-model-config
  namespace: kagent
spec:
  model: tinyllama:latest
  provider: Ollama
  ollama: {}
```

Apply:

```bash
kubectl apply -f ollama-model.yaml
```

Restart controller:

```bash
kubectl rollout restart deployment kagent-controller -n kagent
```

---

# ğŸš€ STEP 5 â€” (Optional) Disable External Providers

If you want fully local mode only:

```bash
helm upgrade kagent oci://ghcr.io/kagent-dev/kagent/helm/kagent \
  -n kagent \
  --reuse-values \
  --set providers.openAI.enabled=false \
  --set providers.gemini.enabled=false
```

Restart controller again:

```bash
kubectl rollout restart deployment kagent-controller -n kagent
```

---

# ğŸš€ STEP 6 â€” Use Ollama in UI

Open UI.

Create or edit agent.

Select:

```
Ollama (tinyllama:latest)
```

Now test:

```
hi
```

It should respond normally.

No 429 errors.
No external API calls.

Fully local.

---

# ğŸ›Ÿ Backup Plan â€” If Something Fails

## Check Ollama Logs

```bash
kubectl logs -n kagent deploy/ollama
```

## Check Controller Logs

```bash
kubectl logs -n kagent deploy/kagent-controller
```

## Check Model Loaded

```bash
kubectl exec -it -n kagent deploy/ollama -- ollama list
```

---

# âš  Common Errors & Fixes

### Error:

```
llama runner process no longer running
```

Cause:

* Not enough RAM

Fix:

* Use tinyllama
* Or upgrade instance to 8GB+

---

### Error:

```
Ollama_chatException - /api/chat
```

Fix:

* Ensure Helm baseUrl is:

```
http://ollama:11434
```

* Restart controller

---
